"""
Syst√®me d'entra√Ænement continu AM√âLIOR√â avec corrections critiques
"""

import os
import sys
import json
import time
import requests
import re
from tqdm import tqdm
from typing import List, Dict

import torch
from torch.utils.data import Dataset, DataLoader
from torch.nn import CrossEntropyLoss
from torch.optim import AdamW
from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts

# Imports locaux
sys.path.append('../Model')
sys.path.append('../Tokenizer')
from gpt2_model import GPT2Model
from Tokenizer import MYBPE


# ============================================
# WIKIPEDIA SCRAPER + Q&A GENERATOR (Am√©lior√©)
# ============================================

class WikipediaScraper:
    def __init__(self, language='fr'):
        self.language = language
        self.api_url = f"https://{language}.wikipedia.org/w/api.php"
        self.headers = {"User-Agent": "WikiQABot/1.0"}

    def get_random_articles(self, count=10):
        print(f"\nüì• R√©cup√©ration de {count} articles Wikipedia...")
        params = {
            'action': 'query',
            'format': 'json',
            'list': 'random',
            'rnnamespace': 0,
            'rnlimit': count
        }
        try:
            response = requests.get(self.api_url, params=params, headers=self.headers, timeout=10)
            response.raise_for_status()
            data = response.json()
            return [{"title": a["title"], "id": a["id"]} for a in data["query"]["random"]]
        except requests.RequestException as e:
            print(f"‚ö†Ô∏è Erreur r√©seau : {e}")
            return []

    def get_article_content(self, title: str) -> Dict:
        params = {
            'action': 'query',
            'format': 'json',
            'titles': title,
            'prop': 'extracts',
            'explaintext': True,
            'exsectionformat': 'plain'
        }
        try:
            response = requests.get(self.api_url, params=params, headers=self.headers, timeout=10)
            response.raise_for_status()
            data = response.json()
            page = list(data['query']['pages'].values())[0]
            if 'extract' not in page:
                return None
            text = self._clean_text(page['extract'])
            return {'title': title, 'content': text, 'length': len(text)}
        except:
            return None

    def _clean_text(self, text: str) -> str:
        text = re.sub(r'\[\d+\]', '', text)
        text = re.sub(r'==+ .*? ==+', '', text)
        text = re.sub(r'\n{2,}', '\n', text)
        text = re.sub(r'\s{2,}', ' ', text)
        return text.strip()


class QAGenerator:
    def __init__(self):
        self.wiki_templates = [
            "Qu'est-ce que {subject} ?",
            "Parle-moi de {subject}.",
            "Explique-moi {subject}.",
            "Que sais-tu sur {subject} ?",
            "D√©cris {subject}.",
        ]
        self.conversation_templates = [
            ("Hello", "Hello! How are you doing today?"),
            ("Hi", "Hi! Great to see you!"),
            ("How are you?", "I'm doing great, thanks for asking!"),
            ("Thank you", "You're very welcome!"),
            ("Good night", "Good night! Sleep well!"),
            ("What's your name?", "I'm a helpful AI assistant."),
            ("Tell me a joke", "Why don't scientists trust atoms? Because they make up everything!"),
        ]

    def _truncate_sentence(self, text: str, max_len=400):
        """CORRECTION : R√©duire la longueur max pour √©viter le overfitting"""
        if len(text) <= max_len:
            return text.strip()
        truncated = text[:max_len]
        end = max(truncated.rfind('.'), truncated.rfind('!'), truncated.rfind('?'))
        if end != -1:
            truncated = truncated[:end + 1]
        return truncated.strip()

    def generate_qa_pairs(self, title: str, content: str, max_pairs=2) -> List[Dict]:
        """CORRECTION : Moins de paires par article, meilleure qualit√©"""
        qa_pairs = []
        paragraphs = [p.strip() for p in content.split('\n') if len(p.strip()) > 150]
        
        # Limiter et varier
        for i, paragraph in enumerate(paragraphs[:max_pairs]):
            question = self.wiki_templates[i % len(self.wiki_templates)].format(subject=title)
            answer = self._truncate_sentence(paragraph, 350)
            if len(answer) > 50:  # Filtrer les r√©ponses trop courtes
                qa_pairs.append({"human": question, "assistant": answer})
        
        return qa_pairs


# ============================================
# DATASET + TRAINING (Am√©lior√©)
# ============================================

class ChatDataset(Dataset):
    def __init__(self, pairs, tokenizer, max_length=256):
        """CORRECTION : R√©duire max_length pour commencer"""
        self.pairs = pairs
        self.tokenizer = tokenizer
        self.max_length = max_length
        
        # Pr√©traiter et filtrer
        self.valid_pairs = []
        for pair in pairs:
            h = pair['human'].strip()
            a = pair['assistant'].strip()
            if len(h) > 0 and len(a) > 0:
                self.valid_pairs.append((h, a))

    def __len__(self):
        return len(self.valid_pairs)

    def __getitem__(self, idx):
        h, a = self.valid_pairs[idx]
        
        # Format am√©lior√©
        text = f"Human: {h}\nAssistant: {a}"
        
        # Tokeniser
        ids = self.tokenizer.encoder(text)
        
        # Truncate si n√©cessaire
        if len(ids) > self.max_length:
            ids = ids[:self.max_length]
        
        # CORRECTION CRITIQUE : Calculer o√π commence la r√©ponse de l'assistant
        prefix = f"Human: {h}\nAssistant:"
        ids_prefix = self.tokenizer.encoder(prefix)
        assist_start = min(len(ids_prefix), len(ids) - 1)
        
        return {
            "input_ids": torch.tensor(ids, dtype=torch.long),
            "assist_start": assist_start,
            "length": len(ids)
        }


def collate_fn(batch, pad_id=0):
    """CORRECTION : Meilleure gestion du padding et des labels"""
    input_ids_list = [b["input_ids"] for b in batch]
    assist_starts = [b["assist_start"] for b in batch]
    max_len = max([t.size(0) for t in input_ids_list])
    
    input_ids = torch.full((len(batch), max_len), pad_id, dtype=torch.long)
    attention_mask = torch.zeros((len(batch), max_len), dtype=torch.long)
    labels = torch.full((len(batch), max_len), -100, dtype=torch.long)
    
    for i, ids in enumerate(input_ids_list):
        L = ids.size(0)
        input_ids[i, :L] = ids
        attention_mask[i, :L] = 1
        
        # CORRECTION : Ne calculer la loss QUE sur la r√©ponse de l'assistant
        start = assist_starts[i]
        if start < L - 1:  # V√©rifier qu'il y a au moins un token de r√©ponse
            labels[i, start:L] = input_ids[i, start:L]
    
    return {
        "input_ids": input_ids, 
        "attention_mask": attention_mask, 
        "labels": labels
    }


# ============================================
# CONTINUOUS TRAINING SYSTEM (Am√©lior√©)
# ============================================

class ContinuousTrainer:
    def __init__(self, model_dir, tokenizer_path, device, language='fr'):
        self.model_dir = model_dir
        self.tokenizer_path = tokenizer_path
        self.device = device
        self.scraper = WikipediaScraper(language)
        self.qa_gen = QAGenerator()
        
        os.makedirs(model_dir, exist_ok=True)
        
        self.model, self.tokenizer, self.config = self._load_or_init_model()
        
        self.history_file = os.path.join(model_dir, "training_history.json")
        self.history = self._load_history()
        
        # AJOUT : Suivre la meilleure loss
        self.best_loss = float('inf')

    def _load_or_init_model(self):
        cfg_path = os.path.join(self.model_dir, "config.json")
        model_path = os.path.join(self.model_dir, "model.pt")
        
        if os.path.exists(cfg_path):
            with open(cfg_path, 'r') as f:
                cfg = json.load(f)
        else:
            # CORRECTION : Config plus adapt√©e pour un petit mod√®le
            cfg = {
                "vocab_size": 5000,
                "embed_dim": 256,      # R√©duit pour √©viter overfitting
                "num_heads": 4,        # R√©duit
                "num_layers": 4,       # R√©duit
                "max_seq_len": 256,    # R√©duit pour commencer
                "dropout": 0.1         # AJOUT : Dropout pour r√©gularisation
            }
            with open(cfg_path, 'w') as f:
                json.dump(cfg, f, indent=2)
        
        tokenizer = MYBPE(vocab_size=cfg["vocab_size"])
        tokenizer.load_tokenizer(self.tokenizer_path)
        
        # CORRECTION : Passer dropout si disponible
        model_kwargs = {
            "vocab_size": cfg["vocab_size"],
            "embed_dim": cfg["embed_dim"],
            "num_heads": cfg["num_heads"],
            "num_layers": cfg["num_layers"],
            "max_seq_len": cfg["max_seq_len"]
        }
        if "dropout" in cfg:
            model_kwargs["dropout"] = cfg["dropout"]
        
        model = GPT2Model(**model_kwargs)
        
        if os.path.exists(model_path):
            print(f"‚úÖ Chargement du mod√®le existant : {model_path}")
            try:
                state = torch.load(model_path, map_location=self.device, weights_only=True)
            except TypeError:
                state = torch.load(model_path, map_location=self.device)
            model.load_state_dict(state)
        else:
            print("üÜï Initialisation d'un nouveau mod√®le")
        
        model.to(self.device)
        return model, tokenizer, cfg

    def _load_history(self):
        if os.path.exists(self.history_file):
            with open(self.history_file, 'r') as f:
                return json.load(f)
        return {"cycles": [], "total_qa_trained": 0, "best_loss": float('inf')}

    def _save_history(self):
        with open(self.history_file, 'w') as f:
            json.dump(self.history, f, indent=2)

    def generate_dataset(self, num_articles=15, qa_per_article=2):
        """CORRECTION : Plus d'articles, moins de QA par article"""
        print("\n" + "="*60)
        print("üîÑ G√âN√âRATION NOUVEAU DATASET")
        print("="*60)
        
        articles = self.scraper.get_random_articles(num_articles)
        dataset = []
        
        for article in tqdm(articles, desc="Articles"):
            data = self.scraper.get_article_content(article['title'])
            if not data or data['length'] < 300:
                continue
            qa_pairs = self.qa_gen.generate_qa_pairs(
                data['title'], data['content'], max_pairs=qa_per_article
            )
            dataset.extend(qa_pairs)
            time.sleep(0.5)  # Rate limiting plus conservateur
        
        # AJOUT : Ajouter des conversations simples (mais pas trop)
        for q, a in self.qa_gen.conversation_templates[:5]:
            dataset.append({"human": q, "assistant": a})
        
        print(f"‚úÖ Dataset g√©n√©r√© : {len(dataset)} paires Q&A")
        return dataset

    def train_on_dataset(self, dataset, epochs=3, batch_size=4, lr=1e-4):
        """CORRECTION : Meilleurs hyperparam√®tres"""
        print("\n" + "="*60)
        print("üöÄ ENTRA√éNEMENT SUR DATASET")
        print("="*60)
        
        if len(dataset) < 10:
            print("‚ö†Ô∏è Dataset trop petit, skip...")
            return [0.0]
        
        # Split train/val (85/15 pour plus de validation)
        split = int(len(dataset) * 0.85)
        train_data = dataset[:split]
        val_data = dataset[split:]
        
        # CORRECTION : max_length ajust√©
        train_ds = ChatDataset(train_data, self.tokenizer, max_length=256)
        val_ds = ChatDataset(val_data, self.tokenizer, max_length=256)
        
        pad_id = getattr(self.tokenizer, "eos_id", 0)
        train_loader = DataLoader(
            train_ds, 
            batch_size=batch_size, 
            shuffle=True, 
            collate_fn=lambda b: collate_fn(b, pad_id)
        )
        val_loader = DataLoader(
            val_ds, 
            batch_size=batch_size, 
            shuffle=False, 
            collate_fn=lambda b: collate_fn(b, pad_id)
        )
        
        # CORRECTION : Learning rate plus faible + scheduler
        optimizer = AdamW(self.model.parameters(), lr=lr, weight_decay=0.01)
        scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=len(train_loader), T_mult=1)
        loss_fn = CrossEntropyLoss(ignore_index=-100)
        
        cycle_losses = []
        
        for ep in range(1, epochs + 1):
            self.model.train()
            total_loss = 0.0
            num_batches = 0
            pbar = tqdm(train_loader, desc=f"Epoch {ep}/{epochs}")
            
            for batch in pbar:
                input_ids = batch["input_ids"].to(self.device)
                labels = batch["labels"].to(self.device)
                attention_mask = batch.get("attention_mask", None)
                if attention_mask is not None:
                    attention_mask = attention_mask.to(self.device)
                
                # CORRECTION : Utiliser attention_mask si le mod√®le le supporte
                try:
                    logits, _ = self.model(input_ids, attention_mask=attention_mask)
                except TypeError:
                    logits, _ = self.model(input_ids)
                
                # CORRECTION : Calculer loss correctement
                lm_logits = logits[:, :-1, :].contiguous()
                lm_labels = labels[:, 1:].contiguous()
                loss = loss_fn(lm_logits.view(-1, lm_logits.size(-1)), lm_labels.view(-1))
                
                # CORRECTION : Gradient clipping
                optimizer.zero_grad()
                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                optimizer.step()
                scheduler.step()
                
                total_loss += loss.item()
                num_batches += 1
                pbar.set_postfix({'loss': f'{loss.item():.4f}', 'lr': f'{scheduler.get_last_lr()[0]:.2e}'})
            
            avg_loss = total_loss / num_batches if num_batches > 0 else 0.0
            cycle_losses.append(avg_loss)
            
            # Validation
            self.model.eval()
            val_loss = 0.0
            val_batches = 0
            with torch.no_grad():
                for batch in val_loader:
                    input_ids = batch["input_ids"].to(self.device)
                    labels = batch["labels"].to(self.device)
                    attention_mask = batch.get("attention_mask", None)
                    if attention_mask is not None:
                        attention_mask = attention_mask.to(self.device)
                    
                    try:
                        logits, _ = self.model(input_ids, attention_mask=attention_mask)
                    except TypeError:
                        logits, _ = self.model(input_ids)
                    
                    lm_logits = logits[:, :-1, :].contiguous()
                    lm_labels = labels[:, 1:].contiguous()
                    loss = loss_fn(lm_logits.view(-1, lm_logits.size(-1)), lm_labels.view(-1))
                    val_loss += loss.item()
                    val_batches += 1
            
            avg_val = val_loss / val_batches if val_batches > 0 else 0.0
            print(f"Epoch {ep} - Train Loss: {avg_loss:.4f} | Val Loss: {avg_val:.4f}")
            
            # AJOUT : Early stopping si val loss explose
            if avg_val > avg_loss * 2:
                print("‚ö†Ô∏è Overfitting d√©tect√©, arr√™t anticip√©")
                break
        
        # Sauvegarder seulement si meilleure loss
        final_loss = cycle_losses[-1] if cycle_losses else float('inf')
        if final_loss < self.best_loss:
            self.best_loss = final_loss
            model_path = os.path.join(self.model_dir, "model.pt")
            torch.save(self.model.state_dict(), model_path)
            print(f"üíæ Meilleur mod√®le sauvegard√© : {model_path} (loss: {final_loss:.4f})")
        
        return cycle_losses

    def run_continuous_training(self, num_cycles=30, articles_per_cycle=15, 
                                qa_per_article=2, epochs=3, batch_size=4, lr=1e-4):
        """CORRECTION : Meilleurs hyperparam√®tres par d√©faut"""
        print("\n" + "="*70)
        print("ü§ñ ENTRA√éNEMENT CONTINU AM√âLIOR√â - D√âMARRAGE")
        print("="*70)
        print(f"üìä Cycles pr√©vus : {num_cycles}")
        print(f"üìö Articles par cycle : {articles_per_cycle}")
        print(f"üí¨ Q&A par article : {qa_per_article}")
        print(f"üîÅ Epochs par cycle : {epochs}")
        print(f"üìâ Learning rate : {lr}")
        print("="*70)
        
        for cycle in range(1, num_cycles + 1):
            print(f"\n\n{'='*70}")
            print(f"üîÑ CYCLE {cycle}/{num_cycles}")
            print(f"{'='*70}")
            
            dataset = self.generate_dataset(articles_per_cycle, qa_per_article)
            
            if not dataset or len(dataset) < 5:
                print("‚ö†Ô∏è Dataset insuffisant, passage au cycle suivant...")
                continue
            
            losses = self.train_on_dataset(dataset, epochs, batch_size, lr)
            
            avg_loss = sum(losses) / len(losses) if losses else 0
            self.history["cycles"].append({
                "cycle": cycle,
                "num_qa": len(dataset),
                "avg_loss": avg_loss,
                "best_loss": self.best_loss,
                "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
            })
            self.history["total_qa_trained"] += len(dataset)
            self.history["best_loss"] = self.best_loss
            self._save_history()
            
            print(f"\n‚úÖ Cycle {cycle} termin√© - Loss: {avg_loss:.4f} | Best: {self.best_loss:.4f}")
            print(f"üìä Total Q&A : {self.history['total_qa_trained']}")
            
            # AJOUT : Pause progressive entre cycles
            if cycle % 5 == 0:
                print(f"‚è∏Ô∏è Pause de 10s apr√®s {cycle} cycles...")
                time.sleep(10)
        
        print("\n" + "="*70)
        print("üéâ ENTRA√éNEMENT TERMIN√â !")
        print(f"üìä {num_cycles} cycles | Best Loss: {self.best_loss:.4f}")
        print("="*70)


# ============================================
# MAIN
# ============================================

def main():
    import argparse
    parser = argparse.ArgumentParser(description="Syst√®me d'entra√Ænement continu AM√âLIOR√â")
    parser.add_argument("--model-dir", type=str, default="./my_tiny_chatbot")
    parser.add_argument("--tokenizer", type=str, default="../Tokenizer/tokenizer_model.bin")
    parser.add_argument("--cycles", type=int, default=30)
    parser.add_argument("--articles", type=int, default=15)
    parser.add_argument("--qa-per-article", type=int, default=2)
    parser.add_argument("--epochs", type=int, default=3)
    parser.add_argument("--batch-size", type=int, default=4)
    parser.add_argument("--lr", type=float, default=1e-4)
    parser.add_argument("--language", type=str, default='en')
    parser.add_argument("--device", type=str, 
                       default="cuda" if torch.cuda.is_available() else "cpu")
    
    args = parser.parse_args()
    
    device = torch.device(args.device)
    
    trainer = ContinuousTrainer(
        model_dir=args.model_dir,
        tokenizer_path=args.tokenizer,
        device=device,
        language=args.language
    )
    
    trainer.run_continuous_training(
        num_cycles=args.cycles,
        articles_per_cycle=args.articles,
        qa_per_article=args.qa_per_article,
        epochs=args.epochs,
        batch_size=args.batch_size,
        lr=args.lr
    )


if __name__ == "__main__":
    main()