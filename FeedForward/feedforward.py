import torch
import torch.nn as nn
import torch.nn.functional as F

class FeedForward(nn.Module):
    """
    Feed-Forward Network (FFN) pour GPT-2
    
    Architecture simple :
    - Linear: embed_dim ‚Üí 4 √ó embed_dim (expansion)
    - GELU activation
    - Linear: 4 √ó embed_dim ‚Üí embed_dim (compression)
    """
    def __init__(self, embed_dim, dropout=0.1):
        """
        Args:
            embed_dim (int): Dimension des embeddings (768 pour GPT-2 small)
            dropout (float): Taux de dropout (0.1 par d√©faut)
        """
        super().__init__()
        
        self.embed_dim = embed_dim
        self.hidden_dim = 4 * embed_dim  # 768 √ó 4 = 3072
        
        # Premi√®re couche lin√©aire (expansion)
        self.fc1 = nn.Linear(embed_dim, self.hidden_dim)
        
        # Deuxi√®me couche lin√©aire (compression)
        self.fc2 = nn.Linear(self.hidden_dim, embed_dim)
        
        # Dropout pour la r√©gularisation
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x):
        """
        Args:
            x: [batch_size, seq_len, embed_dim]
        
        Returns:
            output: [batch_size, seq_len, embed_dim]
        """
        # 1. Expansion : 768 ‚Üí 3072
        x = self.fc1(x)
        
        # 2. Activation GELU
        x = F.gelu(x)
        
        # 3. Dropout
        x = self.dropout(x)
        
        # 4. Compression : 3072 ‚Üí 768
        x = self.fc2(x)
        
        # 5. Dropout final
        x = self.dropout(x)
        
        return x


# ============================================
# TESTS
# ============================================

def test_feedforward():
    """Test du Feed-Forward Network"""
    print("\n" + "="*60)
    print("TEST 1: Feed-Forward Network")
    print("="*60)
    
    # Param√®tres
    batch_size = 2
    seq_len = 10
    embed_dim = 768
    
    # Cr√©er le module
    ffn = FeedForward(embed_dim)
    
    # Input al√©atoire (simule la sortie de l'attention)
    x = torch.randn(batch_size, seq_len, embed_dim)
    
    print(f"‚úì Input shape: {x.shape}")
    
    # Forward pass
    output = ffn(x)
    
    print(f"‚úì Output shape: {output.shape}")
    print(f"‚úì Hidden dimension: {ffn.hidden_dim}")
    
    # V√©rifier que les shapes sont correctes
    assert output.shape == x.shape, "Les shapes ne correspondent pas!"
    print(f"‚úì Shape correcte: {output.shape}")
    
    # Nombre de param√®tres
    num_params = sum(p.numel() for p in ffn.parameters())
    print(f"\n‚úì Nombre de param√®tres: {num_params:,}")
    
    # D√©tails des param√®tres
    fc1_params = ffn.fc1.weight.numel() + ffn.fc1.bias.numel()
    fc2_params = ffn.fc2.weight.numel() + ffn.fc2.bias.numel()
    
    print(f"\nüìä D√©tails des param√®tres:")
    print(f"  - fc1 (768 ‚Üí 3072): {fc1_params:,}")
    print(f"  - fc2 (3072 ‚Üí 768): {fc2_params:,}")
    print(f"  - Total: {num_params:,}")


def test_with_small_dims():
    """Test avec de petites dimensions pour mieux comprendre"""
    print("\n" + "="*60)
    print("TEST 2: FFN avec petites dimensions")
    print("="*60)
    
    # Petites dimensions pour visualiser
    batch_size = 1
    seq_len = 3
    embed_dim = 8
    
    # Cr√©er le module
    ffn = FeedForward(embed_dim, dropout=0.0)  # Pas de dropout pour ce test
    
    # Input simple
    x = torch.ones(batch_size, seq_len, embed_dim)
    
    print(f"‚úì Input shape: {x.shape}")
    print(f"‚úì Hidden dim: {ffn.hidden_dim} (= {embed_dim} √ó 4)")
    
    # Forward
    output = ffn(x)
    
    print(f"‚úì Output shape: {output.shape}")
    print(f"\nüîç Flux des dimensions:")
    print(f"  Input:  {x.shape} ‚Üí [{batch_size}, {seq_len}, {embed_dim}]")
    print(f"  fc1:    [{batch_size}, {seq_len}, {embed_dim}] ‚Üí [{batch_size}, {seq_len}, {ffn.hidden_dim}]")
    print(f"  GELU:   [{batch_size}, {seq_len}, {ffn.hidden_dim}] ‚Üí [{batch_size}, {seq_len}, {ffn.hidden_dim}]")
    print(f"  fc2:    [{batch_size}, {seq_len}, {ffn.hidden_dim}] ‚Üí [{batch_size}, {seq_len}, {embed_dim}]")
    print(f"  Output: {output.shape}")


def test_pipeline_complete():
    """Test du pipeline complet: Attention ‚Üí FFN"""
    print("\n" + "="*60)
    print("TEST 3: Pipeline Attention ‚Üí FFN")
    print("="*60)
    
    # Simuler la sortie de l'attention
    batch_size = 2
    seq_len = 21  # Comme votre exemple "Bonjour, je teste mon GPT-2!"
    embed_dim = 768
    
    # Output de l'attention (simul√©)
    attention_output = torch.randn(batch_size, seq_len, embed_dim)
    
    print(f"‚úì Attention output: {attention_output.shape}")
    
    # Passer dans le FFN
    ffn = FeedForward(embed_dim)
    ffn_output = ffn(attention_output)
    
    print(f"‚úì FFN output: {ffn_output.shape}")
    print(f"\nüéâ Pipeline Attention ‚Üí FFN fonctionne!")


def compare_parameters():
    """Compare les param√®tres Attention vs FFN"""
    print("\n" + "="*60)
    print("COMPARAISON: Attention vs FFN")
    print("="*60)
    
    embed_dim = 768
    
    # Attention (on simule, vous avez d√©j√† le code)
    attention_params = 2_362_368  # Calcul√© dans attention.py
    
    # FFN
    ffn = FeedForward(embed_dim)
    ffn_params = sum(p.numel() for p in ffn.parameters())
    
    print(f"\nüìä Nombre de param√®tres par composant:")
    print(f"  - Multi-Head Attention: {attention_params:,}")
    print(f"  - Feed-Forward Network: {ffn_params:,}")
    print(f"  - Total (1 bloc):       {attention_params + ffn_params:,}")
    
    print(f"\nüîç R√©partition:")
    total = attention_params + ffn_params
    print(f"  - Attention: {attention_params/total*100:.1f}%")
    print(f"  - FFN:       {ffn_params/total*100:.1f}%")
    
    print(f"\nüí° Le FFN contient ~{ffn_params/attention_params:.1f}√ó plus de param√®tres que l'Attention!")


def visualize_gelu():
    """Visualise la fonction GELU vs ReLU"""
    print("\n" + "="*60)
    print("VISUALISATION: GELU vs ReLU")
    print("="*60)
    
    # Cr√©er des valeurs de test
    x = torch.linspace(-3, 3, 13)
    
    # GELU
    gelu_output = F.gelu(x)
    
    # ReLU pour comparaison
    relu_output = F.relu(x)
    
    print("\nüìä Comparaison GELU vs ReLU:\n")
    print("  x     |  GELU  |  ReLU")
    print("--------|--------|-------")
    
    for i in range(len(x)):
        print(f" {x[i]:6.2f} | {gelu_output[i]:6.3f} | {relu_output[i]:6.3f}")
    
    print("\nüí° Observation:")
    print("  - ReLU coupe brutalement √† 0 pour x < 0")
    print("  - GELU a une transition douce (valeurs n√©gatives petites mais non nulles)")
    print("  - GELU est pr√©f√©r√© dans les Transformers modernes")


if __name__ == "__main__":
    print("\nüöÄ TESTS DU FEED-FORWARD NETWORK\n")
    
    # Test 1: FFN basique
    test_feedforward()
    
    # Test 2: Petites dimensions
    test_with_small_dims()
    
    # Test 3: Pipeline complet
    test_pipeline_complete()
    
    # Comparaison avec Attention
    compare_parameters()
    
    # Visualisation GELU
    visualize_gelu()
    
    print("\n" + "="*60)
    print("‚úÖ TOUS LES TESTS PASS√âS!")
    print("="*60)
    print("\nüìÅ Sauvegardez ce fichier dans: FeedForward/feedforward.py")
    print("üéØ Prochaine √©tape: Transformer Block (Semaine 5)")
    print("    ‚Üí Combiner Attention + FFN + LayerNorm + Residual")
    print("="*60 + "\n")