#!/usr/bin/env python3
"""
Script d'entraÃ®nement GPT-2 "mini-large" (~20M paramÃ¨tres)
OptimisÃ© pour CPU / Codespaces
DurÃ©e estimÃ©e: 3â€“5 h sur CPU standard
"""

import torch
import sys
import os
from pathlib import Path
import time

# Ajout des chemins locaux
sys.path.append('./Tokenizer')
sys.path.append('./Training')
sys.path.append('./Model')

from Tokenizer import MYBPE
from training import TextDataset, GPT2Trainer
from gpt2_model import GPT2Model

print("="*60)
print("ğŸš€ ENTRAÃNEMENT GPT-2 (â‰ˆ20M paramÃ¨tres) sur CPU")
print("="*60)

# Configuration "mini-large"
CONFIG = {
    'vocab_size': 300,
    'embed_dim': 256,      # â†‘ Taille d'embedding
    'num_heads': 8,        # â†‘ Nombre de tÃªtes d'attention
    'num_layers': 6,       # â†‘ Nombre de blocs Transformer
    'max_seq_len': 64,
    'batch_size': 2,       # CPU: petit batch
    'num_epochs': 3,
    'learning_rate': 3e-4,
}

print("\nâš™ï¸  Configuration:")
for key, val in CONFIG.items():
    print(f"  {key}: {val}")

# 1. Dataset
print("\nğŸ“¥ TÃ©lÃ©chargement du dataset...")
os.makedirs("data", exist_ok=True)
if not os.path.exists("data/train.txt"):
    print("TÃ©lÃ©chargement de TinyStories...")
    import urllib.request
    url = "https://huggingface.co/datasets/roneneldan/TinyStories/resolve/main/TinyStoriesV2-GPT4-train.txt"
    urllib.request.urlretrieve(url, "data/train.txt")
    print("âœ“ Dataset tÃ©lÃ©chargÃ©")
else:
    print("âœ“ Dataset dÃ©jÃ  prÃ©sent")

# 2. Tokenizer
print("\nğŸ”¤ Tokenizer...")
tokenizer = MYBPE(vocab_size=CONFIG['vocab_size'])

try:
    tokenizer.load_tokenizer("Tokenizer/tokenizer_model.bin")
    print("âœ“ Tokenizer chargÃ©")
except:
    print("âš ï¸  EntraÃ®nement d'un tokenizer...")
    with open("data/train.txt", "r") as f:
        sample = f.read(1_000_000)
    tokenizer = MYBPE(vocab_size=CONFIG['vocab_size'], dataset=sample)
    tokenizer.train_tokenizer()
    tokenizer.build_vocabulary()
    tokenizer.save_tokenizer("Tokenizer/tokenizer_model.bin")
    print("âœ“ Tokenizer entraÃ®nÃ©")

# 3. Dataset rÃ©duit (pour CPU)
print("\nğŸ“š PrÃ©paration du dataset...")
with open("data/train.txt", "r", encoding="utf-8") as f:
    text = f.read(4_000_000)  # 4MB â‰ˆ suffisant pour test CPU

train_dataset = TextDataset(text, tokenizer, seq_len=CONFIG['max_seq_len'])
print(f"âœ“ {len(train_dataset)} sÃ©quences prÃªtes")

# 4. ModÃ¨le GPT-2 (20M)
print("\nğŸ¤– CrÃ©ation du modÃ¨le...")
model = GPT2Model(
    vocab_size=CONFIG['vocab_size'],
    embed_dim=CONFIG['embed_dim'],
    num_heads=CONFIG['num_heads'],
    num_layers=CONFIG['num_layers'],
    max_seq_len=CONFIG['max_seq_len']
)

num_params = sum(p.numel() for p in model.parameters())
print(f"âœ“ ModÃ¨le initialisÃ©: {num_params:,} paramÃ¨tres ({num_params/1e6:.1f}M)")

# 5. Sauvegardes
os.makedirs("checkpoints", exist_ok=True)

# 6. Trainer
print("\nğŸ‹ï¸  Initialisation du Trainer...")
trainer = GPT2Trainer(
    model=model,
    train_dataset=train_dataset,
    learning_rate=CONFIG['learning_rate'],
    batch_size=CONFIG['batch_size'],
    num_epochs=CONFIG['num_epochs'],
    device='cpu',
    checkpoint_dir='./checkpoints'
)

# 7. EntraÃ®nement
print("\n" + "="*60)
print("ğŸš€ DÃ‰BUT DE L'ENTRAÃNEMENT (CPU)")
print("="*60)
print("â±ï¸  EstimÃ©: 3â€“5 heures")
print("ğŸ’¡ Astuce: Fermez tout ce qui consomme CPU dans Codespaces.")
print("="*60 + "\n")

start = time.time()

try:
    trainer.train(save_every=1)
except KeyboardInterrupt:
    print("\nâš ï¸  Interrompu par l'utilisateur")
except Exception as e:
    print(f"\nâŒ Erreur: {e}")

elapsed = time.time() - start

# 8. Test de gÃ©nÃ©ration
print("\n" + "="*60)
print("ğŸ‰ TEST DE GÃ‰NÃ‰RATION")
print("="*60)

model.eval()
prompt = "Once upon a time"
print(f"\nPrompt: '{prompt}'")

tokens = tokenizer.encoder(prompt)
input_ids = torch.tensor([tokens])

with torch.no_grad():
    generated = model.generate(input_ids, max_new_tokens=30, temperature=0.8)

text = tokenizer.decoder(generated[0].tolist())
print(f"\nGÃ©nÃ©rÃ©:\n{text}\n")

# 9. Statistiques
print("="*60)
print("ğŸ“Š STATISTIQUES")
print("="*60)
print(f"âœ“ ParamÃ¨tres: {num_params:,}")
print(f"âœ“ SÃ©quences: {len(train_dataset)}")
print(f"âœ“ Epochs: {CONFIG['num_epochs']}")
if trainer.train_losses:
    print(f"âœ“ Loss initiale: {trainer.train_losses[0]:.4f}")
    print(f"âœ“ Loss finale: {trainer.train_losses[-1]:.4f}")
print(f"âœ“ Temps: {elapsed/60:.1f} minutes")
print("="*60)

# 10. Sauvegarde finale
final_path = './checkpoints/final_model.pt'
torch.save(model.state_dict(), final_path)
print(f"\nâœ“ ModÃ¨le sauvegardÃ©: {final_path}")

print("\n" + "="*60)
print("âœ… TERMINÃ‰!")
print("="*60)
print("\nğŸ’¡ Pour un entraÃ®nement sÃ©rieux:")
print("   â†’ Passe sur Google Colab (GPU)")
print("   â†’ GPU = 50â€“100Ã— plus rapide quâ€™un CPU Codespaces")
print("="*60)
